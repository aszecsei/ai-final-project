\documentclass[screen, authorversion, nonacm, sigconf]{acmart}

\usepackage[linesnumbered,commentsnumbered,ruled]{algorithm2e}

\begin{document}

\title{Exploring the Impact of Decision Tree Depth}

\author{Alic Szecsei}
\affiliation{University of Iowa}
\email{alic-szecsei@uiowa.edu}

\author{Diego Castaneda}
\affiliation{University of Iowa}
\email{diego-castaneda@uiowa.edu}

\author{Willem DeJong}
\affiliation{University of Iowa}
\email{willem-dejong@uiowa.edu}

\begin{abstract}
  Lorem ipsum dolor sit amet, consectetur adipiscing elit. Morbi malesuada, quam in pulvinar varius, metus nunc fermentum urna, id sollicitudin purus odio sit amet enim. Aliquam ullamcorper eu ipsum vel mollis. Curabitur quis dictum nisl. Phasellus vel semper risus, et lacinia dolor. Integer ultricies commodo sem nec semper.
\end{abstract}

%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
\begin{CCSXML}
  <ccs2012>
  <concept>
  <concept_id>10010147.10010257</concept_id>
  <concept_desc>Computing methodologies~Machine learning</concept_desc>
  <concept_significance>500</concept_significance>
  </concept>
  <concept>
  <concept_id>10010147.10010257.10010258.10010259.10010263</concept_id>
  <concept_desc>Computing methodologies~Supervised learning by classification</concept_desc>
  <concept_significance>300</concept_significance>
  </concept>
  <concept>
  <concept_id>10010147.10010257.10010293.10003660</concept_id>
  <concept_desc>Computing methodologies~Classification and regression trees</concept_desc>
  <concept_significance>300</concept_significance>
  </concept>
  <concept>
  <concept_id>10010147.10010257.10010339</concept_id>
  <concept_desc>Computing methodologies~Cross-validation</concept_desc>
  <concept_significance>100</concept_significance>
  </concept>
  </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Machine learning}
\ccsdesc[300]{Computing methodologies~Supervised learning by classification}
\ccsdesc[300]{Computing methodologies~Classification and regression trees}
\ccsdesc[100]{Computing methodologies~Cross-validation}

\keywords{decision trees, model selection}

\maketitle

\section{Background and Motivation}

Construction of decision trees commonly occurs in two phases: first, a ``growing'' phase, in which data is used to expand the decision tree, followed by a ``pruning'' phase, in which noisy or otherwise meaningless nodes are removed from the tree and replaced with leaves. This second phase is used to combat overfitting and eliminate noise.

Additionally, decision tree models can be constrained by size to combat overfitting. Russell and Norvig \cite{russell_norvig_2010} showcase an implementation of restricting a decision tree to be beneath a maximum size by generating the tree in breadth-first fashion, and stopping when the maximum number of nodes has been reached. Garofalakis and Hyun \cite{Garofalakis:2000:EAC:347090.347163} support this type of algorithm, rather than implementing the constraint in the pruning phase: after all, if the program knows a branch of a tree will later be pruned, there is no purpose in constructing that tree branch.

\section{Methods}

The foundation of our decision tree algorithm was the one provided by Russell and Norvig \cite{russell_norvig_2010} which, in turn, is based on the ID3 algorithm \cite{Quinlan1986}.

\begin{function}
  \SetKwFunction{PluralityValue}{Plurality-Value}
  \SetKwFunction{Importance}{Importance}
  \caption{DecisionTreeLearning($examples$, $attributes$, $parent\_examples$, $depth$)}
  \label{algo:DecisionTreeLearning}
  \uIf{$examples$ is empty}{\Return{\PluralityValue{$parent\_examples$}}}
  \uElseIf{$depth = 0$}{\Return{\PluralityValue{$examples$}}}
  \uElseIf{all $examples$ have the same classification $c$}{\Return{a leaf node $c$}}
  \ElseIf{$attributes$ is empty}{\Return{\PluralityValue{$examples$}}}
  $A \gets argmax_{a \in attributes} \Importance{a, examples}$\;
  $tree \gets$ a new decision tree with root test $A$\;
  \ForEach{value $v_k$ of $A$}{
    $exs \gets \left\{e : e \in examples \text{ and } e.A = v_k\right\}$\;
    $subtree \gets \DecisionTreeLearning{exs, attributes - A, examples, depth - 1}$\;
    add a branch to $tree$ with label $\left(A = v_k\right)$ and subtree $subtree$\;
  }
  \Return{$tree$}
\end{function}

We selected four data sets from the Machine Learning Repository at \url{http://archive.ics.uci.edu/ml/datasets.php}. We then ran each through the \ref{algo:DecisionTreeLearning} algorithm with maximum depths ranging from 1 to 10 and performed $k$-fold cross-validation to determine error, with $k = 4$.

\section{Results}

\section{Discussion}

%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
\begin{acks}
This work was supported in part by the University of Iowa.
\end{acks}

\bibliographystyle{ACM-Reference-Format}
\bibliography{tech-report}

\appendix

\section{Research Methods}

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Morbi malesuada, quam in pulvinar varius, metus nunc fermentum urna, id sollicitudin purus odio sit amet enim. Aliquam ullamcorper eu ipsum vel mollis. Curabitur quis dictum nisl. Phasellus vel semper risus, et lacinia dolor. Integer ultricies commodo sem nec semper.

\section{Online Resources}

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Morbi malesuada, quam in pulvinar varius, metus nunc fermentum urna, id sollicitudin purus odio sit amet enim. Aliquam ullamcorper eu ipsum vel mollis. Curabitur quis dictum nisl. Phasellus vel semper risus, et lacinia dolor. Integer ultricies commodo sem nec semper.

\end{document}
\endinput