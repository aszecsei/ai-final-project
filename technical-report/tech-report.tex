\documentclass[screen, authorversion, nonacm, sigconf]{acmart}

\usepackage[linesnumbered,commentsnumbered,ruled]{algorithm2e}

\begin{document}

\title{Exploring the Impact of Decision Tree Depth}

\author{Alic Szecsei}
\affiliation{University of Iowa}
\email{alic-szecsei@uiowa.edu}

\author{Diego Castaneda}
\affiliation{University of Iowa}
\email{diego-castaneda@uiowa.edu}

\author{Willem DeJong}
\affiliation{University of Iowa}
\email{willem-dejong@uiowa.edu}

\begin{abstract}
  Lobortis scelerisque fermentum dui faucibus in ornare quam viverra orci sagittis eu volutpat odio facilisis mauris sit amet massa vitae tortor condimentum lacinia quis vel eros donec ac odio tempor orci dapibus ultrices in iaculis nunc sed augue lacus viverra vitae congue eu consequat ac felis donec et odio pellentesque diam volutpat commodo sed egestas egestas fringilla phasellus faucibus scelerisque eleifend donec pretium vulputate sapien nec sagittis aliquam malesuada bibendum arcu vitae elementum curabitur vitae nunc sed velit dignissim sodales ut eu sem integer vitae justo eget magna fermentum iaculis eu non diam phasellus vestibulum lorem sed risus ultricies tristique nulla aliquet enim tortor at auctor urna nunc id cursus metus aliquam eleifend mi in nulla posuere sollicitudin aliquam ultrices sagittis orci a scelerisque purus semper eget duis at tellus at urna condimentum mattis pellentesque id nibh tortor id aliquet lectus proin nibh nisl condimentum id venenatis a condimentum vitae sapien pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas sed tempus urna et pharetra pharetra massa massa ultricies mi quis hendrerit dolor magna eget est lorem ipsum dolor sit amet consectetur adipiscing elit pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas integer eget aliquet nibh praesent tristique magna sit amet purus gravida quis blandit turpis cursus in hac habitasse platea dictumst quisque sagittis purus sit amet volutpat consequat mauris nunc congue nisi vitae suscipit tellus mauris a diam maecenas sed enim ut sem viverra aliquet eget sit amet tellus cras.
\end{abstract}

%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
\begin{CCSXML}
  <ccs2012>
  <concept>
  <concept_id>10010147.10010257</concept_id>
  <concept_desc>Computing methodologies~Machine learning</concept_desc>
  <concept_significance>500</concept_significance>
  </concept>
  <concept>
  <concept_id>10010147.10010257.10010258.10010259.10010263</concept_id>
  <concept_desc>Computing methodologies~Supervised learning by classification</concept_desc>
  <concept_significance>300</concept_significance>
  </concept>
  <concept>
  <concept_id>10010147.10010257.10010293.10003660</concept_id>
  <concept_desc>Computing methodologies~Classification and regression trees</concept_desc>
  <concept_significance>300</concept_significance>
  </concept>
  <concept>
  <concept_id>10010147.10010257.10010339</concept_id>
  <concept_desc>Computing methodologies~Cross-validation</concept_desc>
  <concept_significance>100</concept_significance>
  </concept>
  </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Machine learning}
\ccsdesc[300]{Computing methodologies~Supervised learning by classification}
\ccsdesc[300]{Computing methodologies~Classification and regression trees}
\ccsdesc[100]{Computing methodologies~Cross-validation}

\keywords{decision trees, model selection}

\maketitle

\section{Background and Motivation}

Machine learning algorithms fall into one of two categories: classification and regression. In regression, input data is consumed and transformed by a function which can produce values along a numeric range; in classification, data is categorized into a finite number of possible values, and machine learning algorithms seek only to label data sets. These classification problems can use both numeric and categorical data attributes. For purely categorical data, decision trees are a way to build machine learning algorithms that are understandable by both humans and machines. Rather than a sort of ``black box'' in which numbers are passed in and answers are returned, there is a logical tree hierarchy that is familiar to anyone who has played the game Twenty Questions.

Decision trees require both a way to construct them, and a way to use them to categorize data. This categorization is a trivial tree traversal, and so many of the innovations regarding decision trees has to do with their construction. Most decision tree construction algorithms use a two-phase approach: first a \emph{growing} phase, followed by a \emph{pruning} phase. In the growing phase, the decision tree is built out, trying to fit the provided training data as closely as possible. To combat over-fitting, the pruning phase determines which branches of the tree are too ``noisy'' using $\chi^2$ tests and removes them.

Additionally, decision tree models can be constrained by size to combat overfitting. Russell and Norvig \cite{russell_norvig_2010} showcase an implementation of restricting a decision tree to be beneath a maximum size by generating the tree in breadth-first fashion, and stopping when the maximum number of nodes has been reached. As stated in Garofalakis, Hyun, Rastogi, and Shim \cite{Garofalakis:2000:EAC:347090.347163}, there is no point in creating a branch when it is guaranteed to be pruned later.

The amount of pruning which occurs is heavily dependent on the data set chosen. As such, the exact values which optimize our trained decision trees are relatively unimportant. However, our goal was to examine how impactful depth-based pruning was, and how much it assisted with reducing overfitting.

\section{Methods}

The foundation of our decision tree algorithm was the one provided by Russell and Norvig \cite{russell_norvig_2010} which, in turn, is based on the ID3 algorithm \cite{Quinlan1986}.

\begin{function}
  \SetKwFunction{PluralityValue}{Plurality-Value}
  \SetKwFunction{Importance}{Importance}
  \caption{DecisionTreeLearning($examples$, $attributes$, $parent\_examples$, $depth$)}
  \label{algo:DecisionTreeLearning}
  \uIf{$examples$ is empty}{\Return{\PluralityValue{$parent\_examples$}}}
  \uElseIf{$depth = 0$}{\Return{\PluralityValue{$examples$}}}
  \uElseIf{all $examples$ have the same classification $c$}{\Return{a leaf node $c$}}
  \ElseIf{$attributes$ is empty}{\Return{\PluralityValue{$examples$}}}
  $A \gets argmax_{a \in attributes} \Importance{a, examples}$\;
  $tree \gets$ a new decision tree with root test $A$\;
  \ForEach{value $v_k$ of $A$}{
    $exs \gets \left\{e : e \in examples \text{ and } e.A = v_k\right\}$\;
    $subtree \gets \DecisionTreeLearning{exs, attributes - A, examples, depth - 1}$\;
    add a branch to $tree$ with label $\left(A = v_k\right)$ and subtree $subtree$\;
  }
  \Return{$tree$}
\end{function}

We selected four data sets from the Machine Learning Repository at \url{http://archive.ics.uci.edu/ml/datasets.php}. We then ran each through the \ref{algo:DecisionTreeLearning} algorithm with maximum depths ranging from 1 to 10 and performed $k$-fold cross-validation to determine error, with $k = 4$.

\section{Results}

\section{Discussion}

%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
\begin{acks}
This work was supported in part by the University of Iowa.
\end{acks}

\bibliographystyle{ACM-Reference-Format}
\bibliography{tech-report}

\appendix

\section{Research Methods}

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Morbi malesuada, quam in pulvinar varius, metus nunc fermentum urna, id sollicitudin purus odio sit amet enim. Aliquam ullamcorper eu ipsum vel mollis. Curabitur quis dictum nisl. Phasellus vel semper risus, et lacinia dolor. Integer ultricies commodo sem nec semper.

\section{Online Resources}

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Morbi malesuada, quam in pulvinar varius, metus nunc fermentum urna, id sollicitudin purus odio sit amet enim. Aliquam ullamcorper eu ipsum vel mollis. Curabitur quis dictum nisl. Phasellus vel semper risus, et lacinia dolor. Integer ultricies commodo sem nec semper.

\end{document}
\endinput